#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Imports Fair Use sources in DJVU  format from www.wbc.poznan.pl

Usage:
  fetch.py [--no-fetch] ID
  fetch.py (-h | --help)
  fetch.py --version

Arguments:
  ID            Publication ID

Options:
  -h --help     Show this screen.
  --version     Show version.
  --no-fetch    Don't fetch, generate index only.
"""

import json
import logging
import os
import re
import subprocess
import tempfile

from docopt import docopt
from lxml import html
import requests

logging.basicConfig(level=logging.DEBUG)


class WBCError(Exception):
    """
    Custom exception
    """
    pass


class WBCFetch(object):
    """
    Fetch WBC publication and issues
    """
    def __init__(self, publication_id, no_fetch=False):
        self._publication_id = publication_id
        self._index_url = "http://www.wbc.poznan.pl/dlibra/publication?id=%d&tab=1" % self._publication_id

        self._no_fetch = no_fetch

        self._path = "publications/%d" % self._publication_id

        self._logger = logging.getLogger("wbc")
        self._session = requests.Session()

        self._logger.debug("Index URL: %s", self._index_url)

        self._index = {
            "index": self._index_url,
            "name": '',
            "copyrights": '',
            "years": [],
            "issues": [],
            "count": 0
        }

        # prepare directories
        self.make_dir("/issues")

    def get_path(self):
        """
        Get storage path
        """
        return self._path

    def get_json_index(self):
        """
        Get index structure of the publication
        """
        return self._index

    def get_html_index(self):
        """
        Get HTML structure of the publication
        """
        result = [
            '<!doctype html>',
            '<head>',
            u'<title>WBC: %s</title>' % self._index['name'],
            '<style>',
            'li {margin: 0.5em 0}'
            '</style>',
            '</head>',
            '<body>',
            '<h1>%s</h1>' % self._index['name'],
            u'<p>Archiwum <a href="%s">materiałów udostępnionych</a> przez WBC na <strong>licencji Fair Use</strong></p>'
            % self._index['index'].replace('&', '&amp;'),
        ]
        last_year = False

        for issue in self._index['issues']:
            if last_year != issue['year']:
                if last_year is not False:
                    result.append('</ul>')

                result.append('<h3 id="%s">%s</h3>' % (issue['year'], issue['year']))
                result.append('<ul>')
                last_year = issue['year']

            result.append(
                u'\t<li><a href="%s">%s</a><br><small><tt>ID %d</tt> / <a href="%s">źródło</a></small></li>' %
                (
                    './issues/%s/%s.txt' % (issue['year'], issue['id']),
                    issue['name'],
                    issue['id'],
                    issue['djvu']
                )
            )

        # the footer
        if last_year is not False:
            result.append('</ul>')

        result.append('<hr><address>Generated by <a href="https://github.com/macbre/wbc">fetch.py</a></address>')

        # end of result
        result.append('</body></html>')

        # @see http://stackoverflow.com/a/9942822
        return u'\n'.join(result).strip()

    def get_publication_id(self):
        """
        Get publication ID
        """
        return self._publication_id

    def make_dir(self, suffix):
        """
        os.makedirs wrapper
        """
        try:
            os.makedirs(self.get_path() + "/" + suffix)
        except OSError:
            pass

    def fetch_and_parse(self, url):
        """
        Fetches provided URL and parses to the tree
        """
        resp = self._session.get(url)

        if resp.status_code != 200:
            raise WBCError("HTTP request <%s> returned status code %d", url, resp.status_code)

        return html.fromstring(resp.text)

    def run(self):
        """
        Do the stuff ;)
        """
        tree = self.fetch_and_parse(self._index_url)

        name = tree.xpath('//h2')[0].text.strip()
        try:
            copyrights = tree.xpath('//a[contains(@href, "671_1")]')[0].text
        except IndexError:
            copyrights = 'no info'

        items = tree.xpath('//div[@id="struct"]/ul//ul/li/a[@class="item-content"]')

        self._logger.debug("%s (%s)", name, copyrights)
        self._logger.debug("Got %d year(s) of issues", len(items))

        self._index['name'] = name
        self._index['copyrights'] = copyrights

        # roczniki (od najstarszych)
        years = []

        for item in reversed(items):
            year = item.text.strip()
            url = item.attrib.get('href')

            year = re.sub('[^0-9]', '_', year)

            self._index['years'].append({
                "year": year,
                "index": url
            })

            years.append((year, url))

        # numery
        for year, url in years:
            self._logger.debug("Year %s: <%s>", year, url)

            # przygotuj strukturę katalogów
            self.make_dir("/issues/" + year)

            tree = self.fetch_and_parse(url)

            # linki do <http://www.wbc.poznan.pl/dlibra/editions-content?id=129941>
            items = tree.xpath('//a[@class="contentTriggerStruct"]')

            self._index['count'] += len(items)

            for item in reversed(items):
                name = item.attrib.get('title', '').strip(' -')
                url = item.attrib.get('href')

                # pobierz ID pliku dla danego numeru
                resp = self._session.get(url)
                issue_id = int(re.search('content_url=/Content/(\d+)/index.djvu', resp.text).group(1))

                djvu_url = "http://www.wbc.poznan.pl/Content/%d/index.djvu" % issue_id
                zip_url = "http://www.wbc.poznan.pl/Content/%d/zip/" % issue_id

                self._logger.debug("%s: <%s>", name, djvu_url)

                self._index['issues'].append({
                    "year": year,
                    "name": name,
                    "id": issue_id,
                    "djvu": djvu_url,
                    "zip": zip_url,
                })

                # --no-fetch
                if self._no_fetch is True:
                    continue

                # pobierz archiwum, rozpakuj i wygeneruj plik txt z treścią
                tmp_dir = tempfile.mkdtemp(prefix="wbc")
                cmd = ['./djvuzip2txt.sh', zip_url, tmp_dir]

                self._logger.debug("cmd: %s", cmd)

                # @see https://docs.python.org/2/library/subprocess.html#subprocess.call
                with open("%s/issues/%s/%d.txt" % (self.get_path(), year, issue_id), "wb") as output:
                    output.writelines([
                        '# Fetched from %s\n' % djvu_url,
                        '# Under Fair Use license\n',
                        '\n'
                    ])

                    output.flush()

                    self._logger.debug("Output: %s", output.name)

                    process = subprocess.Popen(cmd, stdout=output)
                    process.wait()

                    output.flush()

                    if process.returncode != 0:
                        raise Exception("Command failed!")


def run(args):
    """
    Execute WBCFetch with provided arguments
    """
    wbc = WBCFetch(publication_id=int(args['ID']), no_fetch=args['--no-fetch'])

    # run the scraper and covert DJVU files to plain text
    wbc.run()

    # store issues index as JSON
    with open("%s/index.json" % wbc.get_path(), "w") as out:
        json.dump(wbc.get_json_index(), out, indent=2, separators=(',', ': '), sort_keys=True)

    # store issues index as HTML
    with open("%s/index.html" % wbc.get_path(), "w") as out:
        out.write(wbc.get_html_index().encode('utf8'))

if __name__ == '__main__':
    run(docopt(__doc__, version='WBC v0.1'))
