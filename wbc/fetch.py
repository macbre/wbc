#!/usr/bin/env python
# -*- coding: utf-8 -*-
import logging
import os
import re
import subprocess
import tempfile

from lxml import html
import requests


class WBCError(Exception):
    """
    Custom exception
    """
    pass


class WBCFetch(object):
    """
    Fetch WBC publication and issues
    """
    def __init__(self, publication_id, no_fetch=False):
        self._publication_id = publication_id
        self._index_url = "http://www.wbc.poznan.pl/dlibra/publication?id=%d&tab=1" % self._publication_id

        self._no_fetch = no_fetch

        self._path = "publications/%d" % self._publication_id

        self._logger = logging.getLogger("wbc")
        self._session = requests.Session()

        self._logger.debug("Index URL: %s", self._index_url)

        self._index = {
            "index": self._index_url,
            "name": '',
            "copyrights": '',
            "years": [],
            "issues": [],
            "count": 0
        }

        # prepare directories
        self.make_dir("/issues")

    def get_path(self):
        """
        Get storage path
        """
        return self._path

    def get_json_index(self):
        """
        Get index structure of the publication
        """
        return self._index

    def get_html_index(self):
        """
        Get HTML structure of the publication
        """
        result = [
            '<!doctype html>',
            '<head>',
            u'<title>WBC: %s</title>' % self._index['name'],
            '<style>',
            'li {margin: 0.5em 0}'
            '</style>',
            '</head>',
            '<body>',
            '<h1>%s</h1>' % self._index['name'],
            u'<p>Archiwum <a href="%s">materiałów udostępnionych</a>' +
            ' przez WBC na <strong>licencji Fair Use</strong></p>'
            % self._index['index'].replace('&', '&amp;'),
        ]
        last_year = False

        for issue in self._index['issues']:
            if last_year != issue['year']:
                if last_year is not False:
                    result.append('</ul>')

                result.append('<h3 id="%s">%s</h3>' % (issue['year'], issue['year']))
                result.append('<ul>')
                last_year = issue['year']

            result.append(
                u'\t<li><a href="%s">%s</a><br><small><tt>ID %d</tt> / <a href="%s">źródło</a></small></li>' %
                (
                    './issues/%s/%s.txt' % (issue['year'], issue['id']),
                    issue['name'],
                    issue['id'],
                    issue['djvu']
                )
            )

        # the footer
        if last_year is not False:
            result.append('</ul>')

        result.append('<hr><address>Generated by <a href="https://github.com/macbre/wbc">fetch.py</a></address>')

        # end of result
        result.append('</body></html>')

        # @see http://stackoverflow.com/a/9942822
        return u'\n'.join(result).strip()

    def get_publication_id(self):
        """
        Get publication ID
        """
        return self._publication_id

    def make_dir(self, suffix):
        """
        os.makedirs wrapper
        """
        try:
            os.makedirs(self.get_path() + "/" + suffix)
        except OSError:
            pass

    def fetch_and_parse(self, url):
        """
        Fetches provided URL and parses to the tree

        :type url str
        :rtype lxml.html.HtmlElement
        """
        resp = self._session.get(url)

        if resp.status_code != 200:
            raise WBCError("HTTP request <%s> returned status code %d", url, resp.status_code)

        return html.fromstring(resp.text)

    def _get_issues_from_url(self, url):
        """
        Gets the list of all issues from given URL with year index

        May make additional HTTP requests if the list of issues is nested

        @see http://www.wbc.poznan.pl/dlibra/publication?id=86859&tab=3
        """
        tree = self.fetch_and_parse(url)

        # links the issues, e.g. <http://www.wbc.poznan.pl/dlibra/editions-content?id=129941>
        items = tree.xpath('//a[@class="contentTriggerStruct"]')

        # all issues were found, return them
        if len(items) > 0:
            self._logger.debug('<%s> issues found - %s', url, len(items))
            return items

        # we need to recursively fetch the list of all issues
        items = []
        nodes = tree.xpath('//li[a[@href="%s"]]//li/a[@class="item-content"]' % url)

        self._logger.debug('<%s> entering recursive mode', url)

        for node in nodes:
            items.extend(self._get_issues_from_url(node.attrib.get('href')))

        return items

    def run(self):
        """
        Do the stuff ;)
        """
        tree = self.fetch_and_parse(self._index_url)

        name = tree.xpath('//h2')[0].text.strip()
        try:
            copyrights = tree.xpath('//a[contains(@href, "671_1")]')[0].text
        except IndexError:
            copyrights = 'no info'

        items = tree.xpath('//div[@id="struct"]/ul//ul/li/a[img]')

        self._logger.debug("%s (%s)", name, copyrights)
        self._logger.debug("Got %d year(s) of issues", len(items))

        self._index['name'] = name
        self._index['copyrights'] = copyrights

        # roczniki (od najstarszych)
        years = []

        for item in reversed(items):
            year = item.attrib.get('title').rstrip(' -')
            url = item.attrib.get('href')

            if not re.match(r'^\d{4}', year):
                continue

            year = re.sub('[^0-9]', '_', year)

            self._index['years'].append({
                "year": year,
                "index": url
            })

            years.append((year, url))

        # numery
        for year, url in years:
            self._logger.debug("Year %s: <%s>", year, url)

            # przygotuj strukturę katalogów
            self.make_dir("/issues/" + year)

            items = self._get_issues_from_url(url)

            self._index['count'] += len(items)

            for item in reversed(items):
                name = item.attrib.get('title', '').strip(' -')
                url = item.attrib.get('href')

                # pobierz ID pliku dla danego numeru
                resp = self._session.get(url)

                matches = re.search('content_url=/Content/(\d+)/(\w+)\.djvu', resp.text)
                issue_id = int(matches.group(1))
                djvu_file = matches.group(2)

                djvu_url = "http://www.wbc.poznan.pl/Content/%d/%s.djvu" % (issue_id, djvu_file)
                zip_url = "http://www.wbc.poznan.pl/Content/%d/zip/" % issue_id

                self._logger.debug("%s: <%s>", name, djvu_url)

                self._index['issues'].append({
                    "year": year,
                    "name": name,
                    "id": issue_id,
                    "djvu": djvu_url,
                    "zip": zip_url,
                })

                # --no-fetch
                if self._no_fetch is True:
                    continue

                # pobierz archiwum, rozpakuj i wygeneruj plik txt z treścią
                tmp_dir = tempfile.mkdtemp(prefix="wbc")
                cmd = ['./djvuzip2txt.sh', zip_url, tmp_dir]

                self._logger.debug("cmd: %s", cmd)

                # @see https://docs.python.org/2/library/subprocess.html#subprocess.call
                with open("%s/issues/%s/%d.txt" % (self.get_path(), year, issue_id), "wb") as output:
                    """
                    output.writelines([
                        '# Fetched from %s\n' % djvu_url,
                        '# Under Fair Use license\n',
                        '\n'
                    ])
                    output.flush()
                    """

                    self._logger.debug("Output: %s", output.name)

                    process = subprocess.Popen(cmd, stdout=output)
                    process.wait()

                    output.flush()

                    if process.returncode != 0:
                        raise Exception("Command failed!")
